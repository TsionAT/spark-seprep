# Final Class Presentation - December 4th 2024

## Run whisper, an LLM powered application locally

Follow the directions from the Podman AI Lab documentation to [start a whisper application](https://podman-desktop.io/docs/ai-lab/start-recipe).
The `Whisper` applications will use the whisper model, which is different from the model we downloaded in class.
Follow the instructions in podman-desktop to download the whisper model and start the application, it should be very similar to what we did for RAG and chatbot.

The whisper application does speech recognition for multiple languages. You can find some sample audio here https://github.com/redhat-et/whisper-self-hosted-llm/tree/main/data.
However, for the presentation you should use one of your own audio files - make sure it is in the correct format that whisper can understand!

To know more about whisper, look at https://github.com/openai/whisper.

The whisper model on podman-desktop AI Labs is a much smaller version of the actual whisper model so all functionality might not be avaiable.

The documentation view of AI Lab is slightly different than the most up-to-date version you all have on your laptops, but you should be able to easily
deduce how to launch a recipe by finding the `More Details` -> `Start` button.

In class on December 4th, you will show the class your model running with podman-desktop and the whisper application doing speech recognition on the audio file you provide it.
No slideshow is needed and the presentation will be graded. No other submission is needed.

All the best and have fun! Extra credit for those that try out some interesting audio files!
